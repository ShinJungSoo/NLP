{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * 정제 : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.\n",
    "\n",
    "### * 정규화 : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다.\n",
    "\n",
    "- 정제 작업은 토큰화 작업에 방해가 되는 부분들을 배제시키고 토큰화 작업을 수행하기 위해서 토큰화 작업보다 앞서 이루어지지만, 작업 이후에도 남아있는 노이즈들을 제거하기위해서 지속적으로 이루어지기도 합니다.\n",
    "\n",
    "1. 규칙에 기반한 표기가 다른 단어들의 통합\n",
    " > 필요에 따라 직접 코딩을 통해 정의할 수 있는 정규화 규칙의 예로서 같은 의미를 갖고있음에도, 표기가 다른 단어들을 하나의 단어로 정규화하는 방법을 사용할 수 있다.\n",
    " >> 예를 들어 USA와 US는 같은 의미를 가지므로, 하나의 단어로 정규해볼수 있다.\n",
    "\n",
    "2. 대, 소문자 통합\n",
    " > 결국에는 예외 사항을 크게 고려하지 않고, 모든 코퍼스를 소문자로 바꾸는 것이 종종 더 실용적인 해결책이 되기도 합니다. (영어 처리에서는)\n",
    "\n",
    "3. 불필요한 단어의 제거\n",
    " - 정제 작업에서 제거해야하는 노이즈 데이터(noise data)는 자연어가 아니면서 아무 의미도 갖지 않는 글자들(특수 문자 등)을 의미하기도 하지만, 분석하고자 하는 목적에 맞지 않는 불필요 단어들을 노이즈 데이터라고 한다.\n",
    " > 등장 빈도가 적은 단어\n",
    "  - 데이터에서 너무 적게 등장해서 자연어 처리에서 도움이 되지않는 단어들이 존재한다. \n",
    " > 길이가 짧은 단어\n",
    "  - 한국어에는 확실히 영어보다 단어의 길이가 확실히 짧다. ex) 용 / dragon 등등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
    "import re\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "print(shortword.sub('', text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
