{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Model\n",
    "===\n",
    "* 언어 모델은 언어라는 현상을 모델링하고자 단어 시퀀스(또는 문장)에 확률을 할당하는 모델\n",
    "* 만드는 방법은 '통계를 이용한 방법' 과 '인공 신경망을 이용한 방법'으로 구분할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 언어 모델 (Language Model)\n",
    "---\n",
    "* 언어 모델은 단어 시퀀스에 확률을 할당하는 일을 하는 모델\n",
    "* 즉, 가장 자연스러운 단어 시퀀스를 찾아내는 모델\n",
    "* 가장 보편적으로 사용되는 방법은 언어 모델이 이전 단어들이 주어졌을 때 다음 단어를 예측하도록 하는 것\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 단어 시퀀스의 확률 할당\n",
    "---\n",
    "* 자연어 처리에 단어 시퀀스에 확률을 할당하는 일이 왜 필요할까? (P는 확률을 의미)\n",
    "\n",
    "a. 기계 번역 (Machine Translation):\n",
    "\n",
    "    P(나는 버스를 탔다.) > P(나는 버스를 태운다)\n",
    "    : 언어모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단한다.\n",
    "\n",
    "b. 오타 교정(Spell Correction):\n",
    "\n",
    "    선생님이 교실로 부리나케\n",
    "    P(달려갔다) > P(잘려갔다)\n",
    "    : 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단한다.\n",
    "\n",
    "c. 음성 인식(Speech Recognition):\n",
    "\n",
    "    P(나는 메롱을 먹는다) < P(나는 메론을 먹는다)\n",
    "    : 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단한다.\n",
    "* 언어 모델은 위와 같이 확률을 통해 보다 적절한 문장을 판단한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 주어진 이전 단어들로부터 다음 단어 예측하기\n",
    "---\n",
    "* 조건부 확률로 표현\n",
    "\n",
    "A. 단어 시퀀스의 확률\n",
    "\n",
    "    하나의 단어를 w, 단어 시퀀스를 대문자  W라고 한다면, n개의 단어가 등장하는 단어 시퀀스W의 확률은 다음과 같다.\n",
    "                                P(W) = P(w_1, w_2, w_3, w_4, w_5, ... ,w_n)\n",
    "\n",
    "B. 다음 단어 등장 확률\n",
    "    \n",
    "    다음 단어 등장 확률을 식으로 표현, n-1개의 단어가 나열된 상태에서 n번째 단어의 확률은 다음과 같습니다.\n",
    "                                        P(w_n | w_1, ..., w_{n-1})\n",
    "    |의 기호는 조건부확률을 의미\n",
    "    예를 들어 다섯번째 단어의 확률을 구하자면,\n",
    "                                        P(w_5 | w_1, w_2, w_3, w_4)\n",
    "    전체 단어 시퀀스W의 확률은 모든 단어가 예측되고 나서야 알 수 있으므로 단어 시퀀스의 확률은 \n",
    "            P(W) = P(w_1, w_2, w_3, w_4, w_5, ... w_n) = \\prod_{i=1}^{n}P(w_{n} | w_{1}, ... , w_{n-1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "통계적 언어 모델 (Statistical Language Model, SLM)\n",
    "===\n",
    "* 앞에서는 LM의 개념에 대해서 간략히 보았다.\n",
    "* 여기서는 언어모델의 전통적인 접근 방법인 통계적 언어 모델을 소개한다.\n",
    "* 통계적인 접근 방법으로 어떻게 언어를 모델링하는지 알아본다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 조건부 확률\n",
    "---\n",
    "\n",
    "* 조건부 확률은 두 확률 P(A), P(B)에 대해서 아래와 같은 관계를 가진다.\n",
    "    \n",
    "        p(B|A) = P(A,B)/P(A)\n",
    "        P(A,B) = P(A)P(B|A)\n",
    "* 4개의 확률이 조건부 확률을 가질 때, 아래와 같이 표현할 수 있다.\n",
    "\n",
    "        P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)\n",
    "* 이를 조건부 확률의 연쇄법칙(Chain Rule)이라고 한다. 아래는 4개가 아닌 일반화 표현이다.\n",
    "\n",
    "        P(x_1, x_2, x_3 ... x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)...P(x_n|x_1 ... x_{n-1})\n",
    "\n",
    "2. 문장에 대한 확률\n",
    "---\n",
    "* 문장 'An adorable little boy is spreading smiles'이 있다.\n",
    "* 위의 문장의 확률 P는 P(An adorable little boy is spreading smiles)를 식으로 표현해본다.\n",
    "\n",
    "        P(w_1, w_2, w_3, w_4, w_5, ... w_n) = \\prod_{n=1}^{n}P(w_{n} | w_{1}, ... , w_{n-1})\n",
    "* P(\\text{An adorable little boy is spreading smiles}) =\n",
    "     * P(\\text{An})  ×  P(\\text{adorable|An})  ×  P(\\text{little|An adorable})  ×  P(\\text{boy|An adorable little})×  P(\\text{is|An adorable little boy})\n",
    "     * 문장의 확률을 구하기 위해서 각 단어에 대한 예측 확률들을 곱한다.\n",
    " \n",
    "3. 카운트 기반의 접근\n",
    "---\n",
    "* SLM은 이전 단어로부터 어떻게 다음 단어에 대한 확률을 구할까? 정답은 카운트에 기반하여 확률을 계산한다.\n",
    "* An adorable little boy가 나왔을 때, is가 나올 확률인 P(is|An adorable little boy)를 구해본다.\n",
    "        \n",
    "        P(is|An adorable little boy) = count(An adorable little boy is) / count(An adorable little boy)\n",
    "* 확률은 위와 같다. 예를 들어, 학습한 데이터에 An adorable little bot가 100번 등장했는데 그 다음 is가 등장한 경우는 30번 이라고하면 P(is|an adorable little boy)는 30%라고 할 수 있다.\n",
    "\n",
    "4. 카운트 기반 접근의 한계 - 희소 문제(Sparsity Problem)\n",
    "---\n",
    "* 카운트 기반으로 접근하려고 한다면 갖고 있는 코퍼스. 즉, 훈련하는 데이터는 정말 방대한 양이 필요하다.\n",
    "* 위에서 설명한 P(is|an adorable little boy)를 구할 때, 훈련한 코퍼스에 An adorable little boy is라는 단어 시퀀스가 없었다면 이 단어 시퀀스에 대한 확률은 0이 된다. 아니면 An adorable little boy가 없었다면 분모가 0이 되어서 확률이 정의되지 않는다.\n",
    "* 그래서 정확한 모델링 방법이 아니게 되는데, 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링 하지 못하는 문제를 희소 문제(Sparsity problem)이라고 한다.\n",
    "* 위의 문제를 완화하는 방법으로는 n-gram이나 스무딩, 백오프와 같은 여러가지 일반화 기법이 존재\n",
    "* 결국 이러한 한계로 인해서 언어 모델의 트렌드는 통계적 언어 모델에서 인공 신경망 언어 모델로 넘어가게 된다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
