{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram 언어 모델 (N-gram Language Model)\n",
    "===\n",
    "* n-gram 언어 모델은 여전히 Count에 기반한 통계적 접근은 사용하고 있으므로 SLM의 일종\n",
    "* 다만 앞서 배운 언어 모델과는 달리 이전에 등장한 모든 단어를 고려하는 것이 아니라 일부 단어만 고려하는 접근 방법을 사용한다.\n",
    "\n",
    "\n",
    "1. 코퍼스에서 카운트하지 못하는 경우의 감소\n",
    "---\n",
    "* SLM의 한계는 훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을 수 있다는 점이다.\n",
    "* 확률을 계산하고 싶은 문장이 길어질수록 갖고있는 코퍼스에서 그 문장이 존재하지 않을 가능성도 높다.\n",
    "* 즉, 카운트할 수 없을 가능성이 높다. 그런데 다음과 같이 참고하는 단어들을 줄이면 카운트 할 수 있을 가능성을 높일 수 있다.\n",
    "\n",
    "        P(\\text{is|An adorable little boy}) \\approx\\ P(\\text{is|boy})\n",
    "        P(\\text{is|An adorable little boy}) \\approx\\ P(\\text{is|little boy})\n",
    "\n",
    "* 단어의 확률을 구하고자 기준 단어의 앞 단어를 전부 포함해서 카운트하는 것이 아니라, 앞 단어 중 임의의 개수만 포함해서 카운트하여 근사하자는 것이다. 이렇게 하면 갖고 있는 코퍼스에서 해당 단어의 시퀀스를 카운트할 확률이 높아진다.\n",
    "\n",
    "2. N-gram\n",
    "--- \n",
    "*  임의의 개수를 정하기 위한 기준을 위해 사용하는 것이 n-gram이다.\n",
    "* n개의 연속적인 단어 나열을 의미\n",
    "* 갖고 있는 코퍼스에서 n개의 단어 뭉치 단위를 끊어서 이를 하나의 토큰으로 간주한다. 예를 들어서 문장 'An adorable little boy is spreading smiles'이 있을 때, 각 n에 대해서 n-gram을 전부 구해보면\n",
    "            - unigrams : an, adorable, little, boy, is, spreading, smiles\n",
    "            - bigrams : an adorable, adorable little, little boy, boy is, is        spreading, spreading smiles\n",
    "            - trigrams : an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles\n",
    "            - 4-grams : an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles\n",
    "            \n",
    "* n=1 -> unigram, n=2 -> bigram, n=3 -> trigram, 4이상은 n-gram으로 표현\n",
    "\n",
    "3. N-gram LM의 한계\n",
    "---\n",
    "* 데이터를 어떻게 가정하느냐의 나름이고, 전혀 말이 안되는 문장은 아니지만 n-gram은 뒤의 단어 몇개 만 보다 보니 의도하고 싶은 대로 문장을 끝맺음하지 못하는 경우가 생긴다.\n",
    "* 문장을 읽다 보면 앞 부분과 뒷부분의 문맥이 전혀 연결 안되는 경우도 생길 수 있다.\n",
    "* 즉, 전체 문장을 고려한 언어 모델보다는 정확도가 떨어질 수밖에 없다는 점이다.\n",
    "\n",
    "        (1) 희소문제 (Sparsity Problem)\n",
    "* 문장에 존재하는 앞에 나온 단어를 모두 보는 것보다 일부 단어만을 보는 것으로 현실적으로 코퍼스에서 카운트 할 수 있는 확률을 높일 수는 있었지만, n-gram 언어 모델도 여전히 n-gram에 대한 희소 문제가 존재한다.\n",
    "        \n",
    "        (2) n을 선택하는 것은 trade-off 문제\n",
    "* 앞에서 몇 개의 단어를 볼지 n을 정하는 것은 trade-off가 존재한다.\n",
    "* 임의의 개수인 n을 1보다는 2로 선택하는 것은 거의 대부분의 경우에서 언어 모델의 성능을 높일 수 있다.\n",
    "* n을 크게 선택하면 실제 훈련 코퍼스에서 해당 n-gram을 카운트할 수 있는 확률은 적어지므로 희소 문제는 점점 심각해진다.. 또한 n이 커질수록 모델 사이즈가 커진다는 문제점도 있다. 기본적으로 코퍼스의 모든 n-gram에 대해서 카운트를 해야 하기 때문이다.\n",
    "* n을 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 현실의 확률분포와 멀어진다. 그렇기 때문에 적절한 n을 선택해야 한다. 앞서 언급한 trade-off 문제로 인해 정확도를 높이려면 n은 최대 5를 넘게 잡아서는 안 된다고 권장되고 있다.\n",
    "\n",
    "4. 적용분야(Domain)에 맞는 코퍼스의 수집\n",
    "---\n",
    "* 언어 모델에 사용하는 코퍼스를 해당 도메인의 코퍼스를 사용한다면 당연히 언어 모델이 제대로 된 언어 생성을 할 가능성이 높아진다.\n",
    "* 훈련에 사용된 도메인 코퍼스가 무엇이냐에 따라서 성능이 비약적으로 달라지기 때문\n",
    "\n",
    "5. 인공 신경망을 이용한 언어 모델\n",
    "---\n",
    "* N-gram 언어 모델보다 대체적으로 성능이 우수한 인공 신경망을 이용한 언어 모델이 많이 사용되고 있다. 추후 챕터에서 배울 예정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
